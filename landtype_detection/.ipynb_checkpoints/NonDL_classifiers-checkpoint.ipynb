{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier for Satellite Images Using Non Deep Learning Methods\n",
    "\n",
    "This notebook demonstrates how we construct an image classifier using non deep learning methods.\n",
    "\n",
    "More specifically, it will cover the following topics in order:\n",
    "\n",
    "## Agenda\n",
    "####    1. Data Preparation: converting image data into vector form\n",
    "####    2. Vanilla Random Forest as a benchmark\n",
    "####    3. Data Augmentation to deal with class inbalance\n",
    "####    4. Random Forest with augmented data\n",
    "####    5. XGBoost (with vanilla data?)\n",
    "####    6. Performance comparison with CNN using pretrained InceptionV3\n",
    "####    7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation: converting image data into vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from custom_dset_new import train_val_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/ruoyangzhang/Documents/PythonWorkingDirectory/Assignment_data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = train_val_test_split(data_dir, train_split=0.8, val_split=0.2, test_split = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_train_dirs = [dir for dir in sorted(list(train_data.keys())) if os.path.split(dir)[-1] != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vector(img_dir):\n",
    "    img = cv2.imread(img_dir)\n",
    "    b, g, r = cv2.split(img)\n",
    "    rgb_img = cv2.merge([r, g, b])\n",
    "    rgb_img.shape = (1, 28*28*3)\n",
    "    return(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259200/259200 [02:41<00:00, 1602.37it/s]\n"
     ]
    }
   ],
   "source": [
    "input_images = np.array([convert_to_vector(dir) for dir in tqdm(ordered_train_dirs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images.shape = (input_images.shape[0], input_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([train_data[dir] for dir in ordered_train_dirs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make test data into np.arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_test_dirs = [dir for dir in sorted(list(val_data.keys())) if os.path.split(dir)[-1] != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64800/64800 [00:36<00:00, 1768.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test_images = np.array([convert_to_vector(dir) for dir in tqdm(ordered_test_dirs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape = (test_images.shape[0], test_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array([val_data[dir] for dir in ordered_test_dirs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla Random Forest as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, n_jobs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(input_images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9661574074074074\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24130,     0,     0,     0,     0,     0],\n",
       "       [   13, 11355,     1,     4,     0,    69],\n",
       "       [   94,    30,  1310,    35,   138,    49],\n",
       "       [    1,    56,     0, 14065,    21,   503],\n",
       "       [   18,     0,    50,     2,  2856,     0],\n",
       "       [   21,   625,     1,   461,     1,  8891]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance  , water: 0.3724, trees: 0.1766, road: 0.0256, barren_land: 0.2260, building: 0.0452, grassland: 0.1543\n",
      "---------------\n",
      "precision, water: 0.9939, trees: 0.9411, road: 0.9618, barren_land: 0.9655, building: 0.9469, grassland: 0.9347\n",
      "---------------\n",
      "recall   , water: 1.0000, trees: 0.9924, road: 0.7911, barren_land: 0.9603, building: 0.9761, grassland: 0.8891\n",
      "---------------\n",
      "fscore   , water: 0.9970, trees: 0.9661, road: 0.8681, barren_land: 0.9629, building: 0.9613, grassland: 0.9113\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "bookmark = {0: 'precision', 1: 'recall   ', 2: 'fscore   ', 3: 'support'}\n",
    "class_dict = {0: 'water', 1: 'trees', 2: 'road', 3: 'barren_land', 4: 'building', 5: 'grassland'}\n",
    "train_class_count = Counter(val_data.values())\n",
    "train_class_balance = {k:round(v/sum(train_class_count.values()),4) for k,v in train_class_count.items()}\n",
    "\n",
    "print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format('balance  ',\n",
    "                      class_dict[0], train_class_balance[0],\n",
    "                      class_dict[1], train_class_balance[1],\n",
    "                      class_dict[2], train_class_balance[2],\n",
    "                      class_dict[3], train_class_balance[3],\n",
    "                      class_dict[4], train_class_balance[4],\n",
    "                      class_dict[5], train_class_balance[5]))\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "for i, scores in enumerate(metrics.precision_recall_fscore_support(test_labels, preds)):\n",
    "    if i < 3:\n",
    "        print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format(bookmark[i],\n",
    "                      class_dict[0], scores[0],\n",
    "                      class_dict[1], scores[1],\n",
    "                      class_dict[2], scores[2],\n",
    "                      class_dict[3], scores[3],\n",
    "                      class_dict[4], scores[4],\n",
    "                      class_dict[5], scores[5]))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell, we have some evidence to suspect that the class inbalance is costing us performance. We note the following observations:\n",
    "\n",
    "    1. the class 'road' is grossly underrepresented in the training set, potentially leading to a low recall score and low overal performance (fscore: 0.8681)\n",
    "    \n",
    "    2. curiously, the class 'building', despite being underrepresented in the training set, obtained an acceptable prediction performance, possibly due to its visual distinctiveness\n",
    "    \n",
    "    3. on the contrary, the class 'grassland', despite having a relatively fair representation (15.43%), its recall score is below overal performance (fscore: 0.9113), leading us to believe that the class is harder to distinguish from other classes, especially from 'trees' and 'barren_land'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going forward:\n",
    "\n",
    "The vanilla Random Forest's performance reached a respectable 96.5% accuracy with not excellent but acceptable class-wise performance, notably with minimum engineering. \n",
    "\n",
    "Going forward, we keep its performance as our baseline benchmark.\n",
    "\n",
    "We aim to improve the prediction performance of the model by artifitially balancing out the classes a bit by data augmentation of the 2 worst performing classes:\n",
    "\n",
    "    1. road: 2\n",
    "    2. grassland: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation to deal with class inbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written data augmentation functions (image_transformation.py) which provides the following image transformations:\n",
    "\n",
    "    1. random rotation between -25 and 25 degrees\n",
    "    2. random rotation between 26 and 75 degrees\n",
    "    3. random rotation either -90 or 90 degrees\n",
    "    4. adding random noise to the data\n",
    "    5. horizontal flip\n",
    "    6. vertical flip\n",
    "    7. transpose\n",
    "    8. zoom (maximum 1.4x)\n",
    "    \n",
    "With the 4 options, we can increase the representation of a particular class by 7 fold maximum without going into composite transformations\n",
    "    \n",
    "We will try 2 strategies:\n",
    "    1. Only augmenting the aforementioned 2 classes\n",
    "        a. to increase 8 fold the volume of the class 'road'\n",
    "        b. to increase 2 fold the volume of the class 'grassland'\n",
    "    2. Balancing all classes in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_transformations_new1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(image_dirs, fold):\n",
    "\t\"\"\"\n",
    "\tthis function will augment selected images and return it as a vector\n",
    "\t-----\n",
    "\timage_dirs: list of dirs to the images to transform\n",
    "\tfold: the number of times the data is augmented, max = 8\n",
    "\t\"\"\"\n",
    "\n",
    "\t# check if fold limits are respected\n",
    "\tif fold > 8 or fold < 1:\n",
    "\t\treturn('fold has to be between 1 and 8')\n",
    "\n",
    "\t# convert it to an integer in case where a float was received\n",
    "\tfold = int(fold)\n",
    "\n",
    "\t# establish all the functions to use for augmentation\n",
    "\tfunction_list = [random_rotation_25, random_rotation_75, random_rotation_90, random_noise, horizontal_flip, vertical_flip, transpose, zoom]\n",
    "\n",
    "\t# randomly choose which augmentations to use:\n",
    "\taugmentations_to_use = random.sample(function_list, fold)\n",
    "\n",
    "\t# print augmentation functions to use\n",
    "\tprint('the functions to be used for augmentation are: ')\n",
    "\tfor i, fun in enumerate(augmentations_to_use):\n",
    "\t\tprint(i+1, str(fun).split(' ')[1])\n",
    "\n",
    "\t# create a list to store results\n",
    "\taugmented = []\n",
    "\n",
    "\t# now we augment:\n",
    "\tfor img_dir in tqdm(image_dirs):\n",
    "\t\t# first we read the images\n",
    "\t\timg = cv2.imread(img_dir)\n",
    "\t\tb, g, r = cv2.split(img)\n",
    "\t\trgb_img = cv2.merge([r, g, b])\n",
    "\n",
    "\t\t# augment the image\n",
    "\t\tfor aug in augmentations_to_use:\n",
    "\t\t\taug_img = aug(rgb_img)\n",
    "\t\t\tprint(aug_img.shape)\n",
    "\t\t\taug_img.shape = (1, 3*28*28)\n",
    "\t\t\taugmented.append(aug_img)\n",
    "\n",
    "\t# convert to a nparray\n",
    "\taug_img = np.array(aug_img)\n",
    "\n",
    "\t# reshape the array\n",
    "\taug_img.shape = (aug_img.shape[0], aug_img.shape[2])\n",
    "\n",
    "\treturn(aug_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dirs = [list(train_data.keys())[0], list(train_data.keys())[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the functions to be used for augmentation are: \n",
      "1 random_rotation_25\n",
      "2 horizontal_flip\n",
      "(28, 28, 3)\n",
      "(28, 28, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "incompatible shape for a non-contiguous array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-8da753d7add7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-f8b0c7158250>\u001b[0m in \u001b[0;36mimage_augmentation\u001b[0;34m(image_dirs, fold)\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0maug_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0maug_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                         \u001b[0maugmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: incompatible shape for a non-contiguous array"
     ]
    }
   ],
   "source": [
    "image_augmentation(test_dirs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_list = function_list = [random_rotation_25, random_rotation_75, random_rotation_90, random_noise, horizontal_flip, vertical_flip, transpose, zoom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_rotation_25\n",
      "random_rotation_75\n",
      "random_rotation_90\n",
      "random_noise\n",
      "horizontal_flip\n",
      "vertical_flip\n",
      "transpose\n",
      "zoom\n"
     ]
    }
   ],
   "source": [
    "for fun in fun_list:\n",
    "    print(str(fun).split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data = input_images, label = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective='multi:softmax', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 100, alpha = 10, n_estimators=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf.fit(input_images,labels)\n",
    "\n",
    "preds = xg_reg.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
