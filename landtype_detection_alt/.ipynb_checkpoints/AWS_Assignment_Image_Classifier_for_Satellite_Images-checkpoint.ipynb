{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier for Satellite Images Using Non Deep Learning and Deep Learning Methods\n",
    "\n",
    "__Author__: Ruoyang ZHANG\n",
    "\n",
    "\n",
    "This notebook demonstrates how we construct an image classifier using non deep learning methods.\n",
    "\n",
    "More specifically, it will cover the following topics in order:\n",
    "\n",
    "## Agenda \n",
    "####    1. Data Preparation: converting image data into vector form\n",
    "####    2. Vanilla Random Forest as a benchmark\n",
    "####    3. Data Augmentation to deal with class inbalance\n",
    "####    4. Random Forest with augmented data\n",
    "            4.1 Strategy 1\n",
    "            4.2 Strategy 2\n",
    "            4.3 Strategy 3\n",
    "            4.4 Discussion\n",
    "####    5. Performance comparison with CNN using pretrained InceptionV3\n",
    "####    6. Conclusions\n",
    "####    7. Appendix - all functions and classes contained in seperate .py files\n",
    "            7.1 image_augmentation.py (Random Forest)\n",
    "            7.2 custom_dset_new_alt.py (Random Forest & CNN)\n",
    "            7.3 pretrained_inceptionv3_alt.py (CNN)\n",
    "            7.4 train_alt.py (CNN)\n",
    "            7.5 test_alt.py (CNN)\n",
    "            7.6 execute_training_alt.py (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation: converting image data into vector form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We conver the image files into single dimension vectors of 28 * 28 * 3 dimensions\n",
    "2. We will also convert the image classes into integers using the following scheme:\n",
    "\n",
    "    {'water':0, 'trees':1, 'road':2, 'barren_land': 3, 'building': 4, 'grassland':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from custom_dset_new import train_val_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory to the image data\n",
    "data_dir = '/Users/ruoyangzhang/Documents/PythonWorkingDirectory/Assignment_data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test\n",
    "# we use the function that we created for the Convolution Neural Net\n",
    "# Since there is barely any hyper parameters for with the Random Forest Algorithm, we set val_proportion = 0\n",
    "train_data, val_data, test_data = train_val_test_split(data_dir, train_split=0.8, val_split=0.0, test_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional step to remove unwanted sys files \n",
    "ordered_train_dirs = [dir for dir in sorted(list(train_data.keys())) if os.path.split(dir)[-1] != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert the images to a vector\n",
    "def convert_to_vector(img_dir):\n",
    "    img = cv2.imread(img_dir)\n",
    "    b, g, r = cv2.split(img)\n",
    "    rgb_img = cv2.merge([r, g, b])\n",
    "    rgb_img.shape = (1, 28*28*3)\n",
    "    return(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259200/259200 [02:10<00:00, 1982.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert training images to input data\n",
    "input_images = np.array([convert_to_vector(dir) for dir in tqdm(ordered_train_dirs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape training input data\n",
    "input_images.shape = (input_images.shape[0], input_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training input labels\n",
    "train_labels = np.array([train_data[dir] for dir in ordered_train_dirs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make test data into np.arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional step to remove unwanted sys files \n",
    "ordered_test_dirs = [testdir for testdir in sorted(list(test_data.keys())) if os.path.split(testdir)[-1] != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64800/64800 [00:36<00:00, 1792.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert testing images to input data\n",
    "test_images = np.array([convert_to_vector(dir) for dir in tqdm(ordered_test_dirs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape test input data\n",
    "test_images.shape = (test_images.shape[0], test_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct test input labels\n",
    "test_labels = np.array([test_data[testdir] for testdir in ordered_test_dirs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla Random Forest as a benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opt to use the Random Forest algorithm for the particular mission for the following reasons:\n",
    "\n",
    "    1. Its robust performance\n",
    "    \n",
    "    2. Its lack of hyper parameter tuning, this is vital since the image dataset is large (> 300k images), the training time can be significant and hyperparameter tuning should be minimised given the time constraints\n",
    "    \n",
    "    3. It's quick to train\n",
    "    \n",
    "    4. Excellent free open source implementation (Sklearn)\n",
    "    \n",
    "    5. Simplicity\n",
    "    \n",
    "We note also the disadvantage of the Random Forest algorithm:\n",
    "\n",
    "    1. Its model size can easily get quite large and evaluation can be relatively slow\n",
    "    \n",
    "    2. The lack of interpretability. While decision trees are easy to interpret, a forest is not so much. Random Forest is regarded by some as a blackbox due to the weighting mechanism behind its decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "clf.fit(input_images,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = clf.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966358024691358\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24137,     0,     0,     0,     0,     0],\n",
       "       [   10, 11285,     0,     5,     0,    62],\n",
       "       [   83,    30,  1282,    26,   134,    44],\n",
       "       [    1,    44,     0, 14050,    31,   480],\n",
       "       [   18,     0,    39,     2,  2943,     0],\n",
       "       [   35,   650,     3,   483,     0,  8923]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the confusion matrix\n",
    "metrics.confusion_matrix(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance  , water: 0.3725, trees: 0.1753, road: 0.0247, barren_land: 0.2254, building: 0.0463, grassland: 0.1558\n",
      "---------------\n",
      "precision, water: 0.9939, trees: 0.9397, road: 0.9683, barren_land: 0.9646, building: 0.9469, grassland: 0.9384\n",
      "---------------\n",
      "recall   , water: 1.0000, trees: 0.9932, road: 0.8018, barren_land: 0.9619, building: 0.9803, grassland: 0.8840\n",
      "---------------\n",
      "fscore   , water: 0.9970, trees: 0.9657, road: 0.8772, barren_land: 0.9633, building: 0.9633, grassland: 0.9104\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# its precision, recall and fscore in relation to the proportion of each class in the training data\n",
    "\n",
    "bookmark = {0: 'precision', 1: 'recall   ', 2: 'fscore   ', 3: 'support'}\n",
    "class_dict = {0: 'water', 1: 'trees', 2: 'road', 3: 'barren_land', 4: 'building', 5: 'grassland'}\n",
    "train_class_count = Counter(test_data.values())\n",
    "train_class_balance = {k:round(v/sum(train_class_count.values()),4) for k,v in train_class_count.items()}\n",
    "\n",
    "print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format('balance  ',\n",
    "                      class_dict[0], train_class_balance[0],\n",
    "                      class_dict[1], train_class_balance[1],\n",
    "                      class_dict[2], train_class_balance[2],\n",
    "                      class_dict[3], train_class_balance[3],\n",
    "                      class_dict[4], train_class_balance[4],\n",
    "                      class_dict[5], train_class_balance[5]))\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "for i, scores in enumerate(metrics.precision_recall_fscore_support(test_labels, preds)):\n",
    "    if i < 3:\n",
    "        print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format(bookmark[i],\n",
    "                      class_dict[0], scores[0],\n",
    "                      class_dict[1], scores[1],\n",
    "                      class_dict[2], scores[2],\n",
    "                      class_dict[3], scores[3],\n",
    "                      class_dict[4], scores[4],\n",
    "                      class_dict[5], scores[5]))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell, we have some evidence to suspect that the class inbalance is costing us performance. We note the following observations:\n",
    "\n",
    "    1. the class 'road' is grossly underrepresented in the training set, potentially leading to a low recall score and low overal performance (fscore: 0.8772)\n",
    "    \n",
    "    2. curiously, the class 'building', despite being underrepresented in the training set, obtained an acceptable prediction performance, possibly due to its visual distinctiveness\n",
    "    \n",
    "    3. on the contrary, the class 'grassland', despite having a relatively fair representation (15.58%), its recall score is below overal performance (fscore: 0.9104), leading us to believe that the class is harder to distinguish from other classes, especially from 'trees' and 'barren_land'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going forward:\n",
    "\n",
    "The vanilla Random Forest's performance reached a respectable 96.5% accuracy with not excellent but acceptable class-wise performance, notably with minimum engineering. \n",
    "\n",
    "Going forward, we keep its performance as our baseline benchmark.\n",
    "\n",
    "We aim to improve the prediction performance of the model by artifitially balancing out the classes a bit by data augmentation of the 2 worst performing classes:\n",
    "\n",
    "    1. road: 2\n",
    "    2. grassland: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation to deal with class inbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written data augmentation functions (image_augmentation.py) which provides the following image transformations:\n",
    "\n",
    "    1. random rotation between -25 and 25 degrees\n",
    "    2. random rotation between 26 and 75 degrees\n",
    "    3. random rotation either -90 or 90 degrees\n",
    "    4. adding random noise to the data\n",
    "    5. horizontal flip\n",
    "    6. vertical flip\n",
    "    7. transpose\n",
    "    8. zoom (maximum 1.4x)\n",
    "    \n",
    "With the 8 options, we can increase the representation of a particular class by 8 fold maximum without going into composite transformations\n",
    "    \n",
    "We will try 3 strategies in order to evaluate which data augmentation gives us the most desired result:\n",
    "\n",
    "    1. strategy 1\n",
    "        a. to increase 2 fold the volume of the class 'road'\n",
    "        b. to increase 2 fold the volume of the class 'grassland'\n",
    "    2. Strategy 2\n",
    "        a. to increase 4 fold the volume of the class 'road'\n",
    "        b. to increase 2 fold the volume of the class 'grassland'\n",
    "    3. Strategy 3\n",
    "        a. to increase 3 fold the volume of the class 'road'\n",
    "        b. to increase 2 fold the volume of the class 'grassland'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_augmentation import *\n",
    "\n",
    "# set fold variables for the 2 classes\n",
    "class_2_fold = 2\n",
    "class_5_fold = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first prep the augmented training data and the respective labels for class 2 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first extract the image_dirs for class 2 & 5\n",
    "class_2_dirs = [ordered_train_dirs[i] for i, cls in enumerate(train_labels) if cls == 2]\n",
    "class_2_labels = np.array([2] * len(class_2_dirs) * class_2_fold)\n",
    "class_5_dirs = [ordered_train_dirs[i] for i, cls in enumerate(train_labels) if cls == 5]\n",
    "class_5_labels = np.array([5] * len(class_5_dirs) * class_5_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 146/6593 [00:00<00:08, 723.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the functions to be used for augmentation are: \n",
      "1 random_rotation_75\n",
      "2 random_rotation_90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6593/6593 [00:08<00:00, 807.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# we then augment the data for class 2\n",
    "augmented_class_2 = image_augmentation(class_2_dirs, class_2_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 195/40253 [00:00<00:41, 970.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the functions to be used for augmentation are: \n",
      "1 random_rotation_75\n",
      "2 horizontal_flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40253/40253 [00:39<00:00, 1024.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# we then augment the data for class 5\n",
    "augmented_class_5 = image_augmentation(class_5_dirs, class_5_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input data and labels\n",
    "input_images_aug = np.concatenate((input_images,augmented_class_2,augmented_class_5), axis = 0)\n",
    "train_labels_aug = np.concatenate((train_labels, class_2_labels, class_5_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fold variables for the 2 classes\n",
    "class_2_fold_2 = 4\n",
    "class_5_fold_2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make the labels for the augmented data for class 2\n",
    "class_2_labels_2 = np.array([2] * len(class_2_dirs) * class_2_fold_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6593 [00:00<?, ?it/s]/Users/ruoyangzhang/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "  2%|▏         | 145/6593 [00:00<00:08, 721.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the functions to be used for augmentation are: \n",
      "1 vertical_flip\n",
      "2 horizontal_flip\n",
      "3 random_rotation_90\n",
      "4 zoom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6593/6593 [00:09<00:00, 694.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# we then augment the data for class 2\n",
    "augmented_class_2_2 = image_augmentation(class_2_dirs, class_2_fold_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input data and labels\n",
    "input_images_aug_2 = np.concatenate((input_images,augmented_class_2_2,augmented_class_5), axis = 0)\n",
    "train_labels_aug_2 = np.concatenate((train_labels, class_2_labels_2, class_5_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fold variables for the 2 classes\n",
    "class_2_fold_3 = 3\n",
    "class_5_fold_3 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make the labels for the augmented data for class 2\n",
    "class_2_labels_3 = np.array([2] * len(class_2_dirs) * class_2_fold_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/6593 [00:00<00:16, 399.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the functions to be used for augmentation are: \n",
      "1 random_rotation_90\n",
      "2 random_rotation_75\n",
      "3 horizontal_flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6593/6593 [00:08<00:00, 800.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# we then augment the data for class 2\n",
    "augmented_class_2_3 = image_augmentation(class_2_dirs, class_2_fold_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input data and labels\n",
    "input_images_aug_3 = np.concatenate((input_images,augmented_class_2_3,augmented_class_5), axis = 0)\n",
    "train_labels_aug_3 = np.concatenate((train_labels, class_2_labels_3, class_5_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Random Forest with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "clf_aug = RandomForestClassifier(n_estimators=100, n_jobs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "clf_aug.fit(input_images_aug,train_labels_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds_new = clf_aug.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9700308641975308\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, preds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24137,     0,     0,     0,     0,     0],\n",
       "       [   10, 11285,     0,     5,     0,    62],\n",
       "       [   83,    30,  1282,    26,   134,    44],\n",
       "       [    1,    44,     0, 14050,    31,   480],\n",
       "       [   18,     0,    39,     2,  2943,     0],\n",
       "       [   35,   650,     3,   483,     0,  8923]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the confusion matrix\n",
    "metrics.confusion_matrix(test_labels, preds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance  , water: 0.2726, trees: 0.1288, road: 0.0560, barren_land: 0.1666, building: 0.0338, grassland: 0.3422\n",
      "---------------\n",
      "precision, water: 0.9946, trees: 0.9669, road: 0.9709, barren_land: 0.9823, building: 0.9463, grassland: 0.9072\n",
      "---------------\n",
      "recall   , water: 1.0000, trees: 0.9843, road: 0.7724, barren_land: 0.9460, building: 0.9800, grassland: 0.9453\n",
      "---------------\n",
      "fscore   , water: 0.9973, trees: 0.9755, road: 0.8603, barren_land: 0.9638, building: 0.9629, grassland: 0.9259\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# its precision, recall and fscore in relation to the proportion of each class in the training data\n",
    "\n",
    "bookmark = {0: 'precision', 1: 'recall   ', 2: 'fscore   ', 3: 'support'}\n",
    "class_dict = {0: 'water', 1: 'trees', 2: 'road', 3: 'barren_land', 4: 'building', 5: 'grassland'}\n",
    "train_class_count_aug = Counter(train_labels_aug)\n",
    "train_class_balance_aug = {k:round(v/sum(train_class_count_aug.values()),4) for k,v in train_class_count_aug.items()}\n",
    "\n",
    "print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format('balance  ',\n",
    "                      class_dict[0], train_class_balance_aug[0],\n",
    "                      class_dict[1], train_class_balance_aug[1],\n",
    "                      class_dict[2], train_class_balance_aug[2],\n",
    "                      class_dict[3], train_class_balance_aug[3],\n",
    "                      class_dict[4], train_class_balance_aug[4],\n",
    "                      class_dict[5], train_class_balance_aug[5]))\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "for i, scores in enumerate(metrics.precision_recall_fscore_support(test_labels, preds_new)):\n",
    "    if i < 3:\n",
    "        print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format(bookmark[i],\n",
    "                      class_dict[0], scores[0],\n",
    "                      class_dict[1], scores[1],\n",
    "                      class_dict[2], scores[2],\n",
    "                      class_dict[3], scores[3],\n",
    "                      class_dict[4], scores[4],\n",
    "                      class_dict[5], scores[5]))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "clf_aug_2 = RandomForestClassifier(n_estimators=100, n_jobs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "clf_aug_2.fit(input_images_aug_2,train_labels_aug_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds_new_2 = clf_aug_2.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9695370370370371\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, preds_new_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24136,     0,     1,     0,     0,     0],\n",
       "       [   14, 11182,     1,     2,     0,   163],\n",
       "       [   42,    28,  1455,     8,    35,    31],\n",
       "       [    1,    40,    13, 13828,    23,   701],\n",
       "       [    5,     1,   291,     1,  2702,     2],\n",
       "       [   18,   326,     6,   221,     0,  9523]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the confusion matrix\n",
    "metrics.confusion_matrix(test_labels, preds_new_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance  , water: 0.2628, trees: 0.1241, road: 0.0900, barren_land: 0.1606, building: 0.0326, grassland: 0.3299\n",
      "---------------\n",
      "precision, water: 0.9967, trees: 0.9659, road: 0.8234, barren_land: 0.9835, building: 0.9790, grassland: 0.9139\n",
      "---------------\n",
      "recall   , water: 1.0000, trees: 0.9842, road: 0.9099, barren_land: 0.9467, building: 0.9001, grassland: 0.9434\n",
      "---------------\n",
      "fscore   , water: 0.9983, trees: 0.9749, road: 0.8645, barren_land: 0.9648, building: 0.9379, grassland: 0.9284\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# its precision, recall and fscore in relation to the proportion of each class in the training data\n",
    "\n",
    "bookmark = {0: 'precision', 1: 'recall   ', 2: 'fscore   ', 3: 'support'}\n",
    "class_dict = {0: 'water', 1: 'trees', 2: 'road', 3: 'barren_land', 4: 'building', 5: 'grassland'}\n",
    "train_class_count_aug_2 = Counter(train_labels_aug_2)\n",
    "train_class_balance_aug_2 = {k:round(v/sum(train_class_count_aug_2.values()),4) for k,v in train_class_count_aug_2.items()}\n",
    "\n",
    "print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format('balance  ',\n",
    "                      class_dict[0], train_class_balance_aug_2[0],\n",
    "                      class_dict[1], train_class_balance_aug_2[1],\n",
    "                      class_dict[2], train_class_balance_aug_2[2],\n",
    "                      class_dict[3], train_class_balance_aug_2[3],\n",
    "                      class_dict[4], train_class_balance_aug_2[4],\n",
    "                      class_dict[5], train_class_balance_aug_2[5]))\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "for i, scores in enumerate(metrics.precision_recall_fscore_support(test_labels, preds_new_2)):\n",
    "    if i < 3:\n",
    "        print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format(bookmark[i],\n",
    "                      class_dict[0], scores[0],\n",
    "                      class_dict[1], scores[1],\n",
    "                      class_dict[2], scores[2],\n",
    "                      class_dict[3], scores[3],\n",
    "                      class_dict[4], scores[4],\n",
    "                      class_dict[5], scores[5]))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Strategy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "clf_aug_3 = RandomForestClassifier(n_estimators=100, n_jobs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "clf_aug_3.fit(input_images_aug_3,train_labels_aug_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds_new_3 = clf_aug_3.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9702006172839506\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, preds_new_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24137,     0,     0,     0,     0,     0],\n",
       "       [   16, 11183,     0,     1,     0,   162],\n",
       "       [   50,    30,  1395,    10,    57,    57],\n",
       "       [    1,    42,    10, 13817,    23,   713],\n",
       "       [   15,     0,   170,     1,  2814,     2],\n",
       "       [   17,   327,     4,   223,     0,  9523]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the confusion matrix\n",
    "metrics.confusion_matrix(test_labels, preds_new_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance  , water: 0.2676, trees: 0.1264, road: 0.0734, barren_land: 0.1635, building: 0.0332, grassland: 0.3359\n",
      "---------------\n",
      "precision, water: 0.9959, trees: 0.9655, road: 0.8835, barren_land: 0.9833, building: 0.9724, grassland: 0.9107\n",
      "---------------\n",
      "recall   , water: 1.0000, trees: 0.9842, road: 0.8724, barren_land: 0.9460, building: 0.9374, grassland: 0.9434\n",
      "---------------\n",
      "fscore   , water: 0.9980, trees: 0.9748, road: 0.8779, barren_land: 0.9643, building: 0.9545, grassland: 0.9268\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# its precision, recall and fscore in relation to the proportion of each class in the training data\n",
    "\n",
    "bookmark = {0: 'precision', 1: 'recall   ', 2: 'fscore   ', 3: 'support'}\n",
    "class_dict = {0: 'water', 1: 'trees', 2: 'road', 3: 'barren_land', 4: 'building', 5: 'grassland'}\n",
    "train_class_count_aug_3 = Counter(train_labels_aug_3)\n",
    "train_class_balance_aug_3 = {k:round(v/sum(train_class_count_aug_3.values()),4) for k,v in train_class_count_aug_3.items()}\n",
    "\n",
    "print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format('balance  ',\n",
    "                      class_dict[0], train_class_balance_aug_3[0],\n",
    "                      class_dict[1], train_class_balance_aug_3[1],\n",
    "                      class_dict[2], train_class_balance_aug_3[2],\n",
    "                      class_dict[3], train_class_balance_aug_3[3],\n",
    "                      class_dict[4], train_class_balance_aug_3[4],\n",
    "                      class_dict[5], train_class_balance_aug_3[5]))\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "for i, scores in enumerate(metrics.precision_recall_fscore_support(test_labels, preds_new_3)):\n",
    "    if i < 3:\n",
    "        print('{}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}, {}: {:.4f}'\\\n",
    "              .format(bookmark[i],\n",
    "                      class_dict[0], scores[0],\n",
    "                      class_dict[1], scores[1],\n",
    "                      class_dict[2], scores[2],\n",
    "                      class_dict[3], scores[3],\n",
    "                      class_dict[4], scores[4],\n",
    "                      class_dict[5], scores[5]))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In resume, we have trained 4 models with different data augmentations with the following accuracy performance:\n",
    "\n",
    "Original Vanilla Model: 0.9664\n",
    "\n",
    "Model of strategy 1   : 0.9700\n",
    "\n",
    "Model of strategy 2   : 0.9695\n",
    "\n",
    "Model of strategy 3   : 0.9702\n",
    "\n",
    "We can see a minor increase in model accuracy after various degrees of data augmentation. From an accuracy's point of view, the data augmented models are generally superior.\n",
    "\n",
    "However, we know that accuracy is not a very holistic metric in evaluating a model's performance. We have noted previously that :\n",
    "\n",
    "1. the original model's performance was skewed towards classes with large sample sizes\n",
    "2. the original model's precision/recall balance was weak for the aforementioned classes: 2 & 5\n",
    "\n",
    "This is when the data augmented models performed quite differently:\n",
    "\n",
    "1. model of strategy 1 performed poorly for class 2 and 5, with class 2's precision (0.9709) being largely superior than its recall (0.7724)\n",
    "2. model of strategy 2 had a similar problem with the precision of class 2 (0.8234) being largely inferior to its recall (0.9099)\n",
    "3. model of strategy 3's class-wise precision-recall balance is fairly well upheld\n",
    "\n",
    "\n",
    "Overall, the original model not only lacked in accuracy, it also had class-wise performance inbalance (measured in fscore). The data augmented models performed better in that regard, but model 1 and 2 both had precision-recall balance issues for class 2, whereas model 3 achieved a relatively good level of balance, both in terms of class-wise performance and class-wise precision-recall balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance comparison with CNN using pretrained InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 The CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied the transfer learning technique to avoid retrainning the model from scratch.\n",
    "We used the pretrained InceptionV3 model provided by the PyTorch framework for the following reasons:\n",
    "    1. Its relative light weight for training in comparison with other popular architectures such as Resnet or VGG\n",
    "    2. Its availability in torch.vision (there are other light architectures such as mobilenet but they are not readily available in torch.vision)\n",
    "    3. Its rebust performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained multiple models using this architecture:\n",
    "\n",
    "Shared variables:\n",
    "\n",
    "Optimisation: SGD\n",
    "\n",
    "Loss Function: Cross Entropy\n",
    "\n",
    "Batchsize: 32\n",
    "\n",
    "Group 1, 8 training epochs:\n",
    "    1. fully connected layers unfrozen, decaying learning rate 1e-3, step size = 7, gamma = 0.1\n",
    "    2. layers after 'Conv2d_4a_3x3' unfrozen, decaying learning rate 1e-3, step size = 7, gamma = 0.1\n",
    "Group 2, 5 training epochs:\n",
    "    1. fully connected layers unfrozen & decaying learning rate 1e-5, step size = 3, gamma = 0.2\n",
    "    2. layers after 'Conv2d_4a_3x3' unfrozen, decaying learning rate 1e-5, step size = 3, gamma = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 2 performed better on the validation set with considerably less training data:\n",
    "\n",
    "Best alidation accuracy:\n",
    "\n",
    "    Group 1:\n",
    "        1. 0.8789\n",
    "        2. 0.8827\n",
    "    Group 2:\n",
    "        1. 0.9043\n",
    "        2. 0.9156   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally kept the 2nd model from group 2.\n",
    "\n",
    "We chose not to further tune the hyper parameter or run any more epochs since the Random Forest algorithm provides a much superior performance in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to training being interrupted half way, the model not the testing data was not succesfully saved, please find below a proof of the best performing model (group 2 model 2)'s validation performing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"Screenshot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In conclusion, model from strategy 3 is the most desirable model\n",
    "\n",
    "Model description:\n",
    "\n",
    "Algorithm: Random Forest (SkLearn implementation)\n",
    "\n",
    "Number of estimators: 100\n",
    "\n",
    "Data Augmentation: \n",
    "3 fold data augmentation of class 2 (road):\n",
    "1. random_rotation_90\n",
    "2. random_rotation_75\n",
    "3. horizontal_flip\n",
    "\n",
    "\n",
    "2 fold data augmentation of class 5 (grassland):\n",
    "1. random_rotation_75\n",
    "2. horizontal_flip\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "The model performed well with an __accuray of 0.9702__ with minimum class-wise performance inbalance.\n",
    "\n",
    "Class-wise precision, recall and fscore:\n",
    "\n",
    "__precision__, _water_: 0.9959, _trees_: 0.9655, _road_: 0.8835, _barren_land_: 0.9833, _building_: 0.9724, _grassland_: 0.9107\n",
    "\n",
    "__recall__   , _water_: 1.0000, _trees_: 0.9842, _road_: 0.8724, _barren_land_: 0.9460, _building_: 0.9374, _grassland_: 0.9434\n",
    "\n",
    "__fscore__   , _water_: 0.9980, _trees_: 0.9748, _road_: 0.8779, _barren_land_: 0.9643, _building_: 0.9545, _grassland_: 0.9268\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Appendix - all functions and classes contained in seperate .py files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 image_augmentation.py\n",
    "function used in the code above to perform image data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import util\n",
    "from math import floor\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_rotation_25(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25 on the left and 25 degrees on the right\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return(sk.transform.rotate(image_array, random_degree))\n",
    "\n",
    "def random_rotation_75(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 26 on the right and 75 degrees on the right\n",
    "    random_degree = random.uniform(26, 75)\n",
    "    return(sk.transform.rotate(image_array, random_degree))\n",
    "\n",
    "def random_rotation_90(image_array: ndarray):\n",
    "    # randomly rotate image of 90 degrees either to the left or to the right\n",
    "    random_degree = random.choice([-90, 90])\n",
    "    return(sk.transform.rotate(image_array, random_degree))\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return(sk.util.random_noise(image_array))\n",
    "\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "    # horizontal flip\n",
    "    return(image_array[:, ::-1])\n",
    "\n",
    "def vertical_flip(image_array: ndarray):\n",
    "    # vertical flip\n",
    "    return(image_array[::-1, :])\n",
    "\n",
    "def transpose(image_array: ndarray):\n",
    "\t# transpose the image\n",
    "\treturn(image_array[::-1, ::-1])\n",
    "\n",
    "def zoom(image_array: ndarray):\n",
    "\t# zoom in on the image, maximum zoom: 1.4\n",
    "\tdim = image_array.shape[0]\n",
    "\tzoom_factor = random.uniform(1.01, 1.4)\n",
    "\tzoomed_image = sk.transform.rescale(image_array, zoom_factor)\n",
    "\tcrop_border = floor((zoomed_image.shape[0] - dim)/2)\n",
    "\tcropped_image = zoomed_image[crop_border : crop_border + dim, crop_border : crop_border + dim]\n",
    "\treturn(cropped_image)\n",
    "\n",
    "def image_augmentation(image_dirs, fold):\n",
    "\t\"\"\"\n",
    "\tthis function will augment selected images and return it as a vector\n",
    "\t-----\n",
    "\timage_dirs: list of dirs to the images to transform\n",
    "\tfold: the number of times the data is augmented, max = 8\n",
    "\t\"\"\"\n",
    "\n",
    "\t# check if fold limits are respected\n",
    "\tif fold > 8 or fold < 1:\n",
    "\t\treturn('fold has to be between 1 and 8')\n",
    "\n",
    "\t# convert it to an integer in case where a float was received\n",
    "\tfold = int(fold)\n",
    "\n",
    "\t# establish all the functions to use for augmentation\n",
    "\tfunction_list = [random_rotation_25, random_rotation_75, random_rotation_90, random_noise, horizontal_flip, vertical_flip, transpose, zoom]\n",
    "\n",
    "\t# randomly choose which augmentations to use:\n",
    "\taugmentations_to_use = random.sample(function_list, fold)\n",
    "\n",
    "\t# print augmentation functions to use\n",
    "\tprint('the functions to be used for augmentation are: ')\n",
    "\tfor i, fun in enumerate(augmentations_to_use):\n",
    "\t\tprint(i+1, str(fun).split(' ')[1])\n",
    "\n",
    "\t# create a list to store results\n",
    "\taugmented = []\n",
    "\n",
    "\t# now we augment:\n",
    "\tfor img_dir in tqdm(image_dirs):\n",
    "\t\t# first we read the images\n",
    "\t\timg = cv2.imread(img_dir)\n",
    "\t\tb, g, r = cv2.split(img)\n",
    "\t\trgb_img = cv2.merge([r, g, b])\n",
    "\n",
    "\t\t# augment the image\n",
    "\t\tfor aug in augmentations_to_use:\n",
    "\t\t\taug_img = np.array(aug(rgb_img))\n",
    "\t\t\taug_img.shape = (1, 3*28*28)\n",
    "\t\t\taugmented.append(aug_img)\n",
    "\n",
    "\t# convert to a nparray\n",
    "\taugmented = np.concatenate(augmented, axis = 0)\n",
    "\n",
    "\treturn(augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 custom_dset_alt.py \n",
    "contains custom dataset class as well as a function to split our image dataset into train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import math\n",
    "\n",
    "\n",
    "def train_val_test_split(data_dir, train_split, val_split, test_split):\n",
    "    \"\"\"\n",
    "    Split data set into training, validation, and test sets.\n",
    "    data_dir : path to images folder\n",
    "    train_split : proportion of the data to be used for training\n",
    "    val_split : proportion of the data to be used for validation\n",
    "    test_split : proportion of the data to be used for testing\n",
    "    \"\"\"\n",
    "\n",
    "    # getting the sub folders and getting rid off irrelevant readings\n",
    "    sub_paths = [os.path.join(data_dir, file_dir) for file_dir in os.listdir(data_dir)]\n",
    "    sub_paths = [path for path in sub_paths if os.path.isdir(path)]\n",
    "\n",
    "    # creating a dict to convert string classes into integers that can be converted to tensors\n",
    "    class_dict = {'water':0, 'trees':1, 'road':2, 'barren_land': 3, 'building': 4, 'grassland':5}\n",
    "\n",
    "    # creating a dict for all files stored in the different class specific folders\n",
    "    # the dict contains key-value pairs of the form: full_file_dir: class\n",
    "    all_files_dict = {}\n",
    "    for path in sub_paths:\n",
    "        all_files_dict = {**all_files_dict, **{os.path.join(path, file_name):class_dict[os.path.split(path)[1]] for file_name in os.listdir(path)}}\n",
    "\n",
    "    # now sample according to the proportions\n",
    "    # Size of data set\n",
    "    N = len(all_files_dict)\n",
    "    \n",
    "    # Size of train set\n",
    "    train_size = math.floor(train_split * N)\n",
    "    \n",
    "    # Size of validation set\n",
    "    val_size = math.floor(val_split * N)\n",
    "    \n",
    "    # List of all data indices\n",
    "    indices = list(range(N))\n",
    "    \n",
    "    # Random selection of indices for train set\n",
    "    train_ids = np.random.choice(indices, size=train_size, replace=False)\n",
    "    train_ids = list(train_ids)\n",
    "    \n",
    "    # Deletion of indices used for train set\n",
    "    indices = list(set(indices) - set(train_ids))\n",
    "    \n",
    "    # Random selection of indices for validation set\n",
    "    val_ids = np.random.choice(indices, size=val_size, replace=False)\n",
    "    val_ids = list(val_ids)\n",
    "    \n",
    "    # Selecting remaining indices for test set\n",
    "    test_ids = list(set(indices) - set(val_ids))\n",
    "\n",
    "    # creating subsets in the forms of dictionaries\n",
    "    # these dicts contain key-value pairs of the form 'file_dir : class'\n",
    "    all_files = sorted(list(all_files_dict.keys()))\n",
    "    train_files = {all_files[i]: all_files_dict[all_files[i]] for i in train_ids}\n",
    "    val_files = {all_files[i]: all_files_dict[all_files[i]] for i in val_ids}\n",
    "    test_files = {all_files[i]: all_files_dict[all_files[i]] for i in test_ids}\n",
    "\n",
    "    return(train_files, val_files, test_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class custom_dset(Dataset):\n",
    "    def __init__(self, data_files, transform):\n",
    "        \"\"\"\n",
    "        data_files : one of the outputs of the train_val_test_split function\n",
    "                     a dictionary containing keys of image directories and values of their respective classes\n",
    "        transform : either 'train' or 'val'\n",
    "                    indicates whether the dataset is for training or validation purposees\n",
    "        \"\"\"\n",
    "\n",
    "        # setting the all_file_dict to a class variable\n",
    "        self.dir_to_class_dict = data_files\n",
    "\n",
    "        # setting all file directories to a class variable\n",
    "        self.all_files = sorted(list(data_files.keys()))\n",
    "\n",
    "        # creating a list of all classes in the order of the all_files class variable\n",
    "        self.labels = [data_files[key] for key in self.all_files]\n",
    "\n",
    "        # setting the len variable\n",
    "        self.len = len(self.dir_to_class_dict)\n",
    "        \n",
    "        # setting the image transform class method\n",
    "        normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "        if transform == 'train':\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(299),\n",
    "                transforms.CenterCrop(299),\n",
    "                transforms.ColorJitter(hue = .05, saturation = .05),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(90, resample = Image.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "                ])\n",
    "        elif transform == 'val':\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(299),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "                ])\n",
    "        self.transform = transform\n",
    "\n",
    "    # the __len__ method\n",
    "    def __len__(self):\n",
    "        return(len(self.all_files))\n",
    "\n",
    "    # the __getitem__ method\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.all_files[idx]\n",
    "        image = Image.open(file_name)\n",
    "        # the images are in .png format, which comes with an extra alpha channel\n",
    "        # since we're using a pretrained model,the data has to be converted to 3 channels\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.dir_to_class_dict[file_name]\n",
    "        label_t = torch.from_numpy(np.array(label))\n",
    "        \n",
    "        return (image, label_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 pretrain_inceptionv3_alt.py\n",
    "class for the pretrained torch.vision inception_v3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import math\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "# the pretrained inception v3 model class\n",
    "#----------------------------------------------------\n",
    "\n",
    "class pretrained_inception_v3(nn.Module):\n",
    "\n",
    "\tdef __init__(self, num_class, use_cuda):\n",
    "\t\tsuper(pretrained_inception_v3, self).__init__()\n",
    "\t\t\"\"\"\n",
    "\t\tnum_class : the total number of classes to predict\n",
    "\t\tuse_cude : if we're using the GPU to compute and optimise\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# setting variables for if we're using the GPU, the number of output classes as well as the tensor dtype\n",
    "\t\tself.use_cuda = use_cuda\n",
    "\t\tself.num_class = num_class\n",
    "\t\tself.dtype = torch.cuda.FloatTensor if self.use_cuda else torch.FloatTensor\n",
    "\n",
    "\t\t# we're using the pretrained inceptionv3 model provided by torchvision\n",
    "\t\tmodel = models.inception_v3(pretrained=True)\n",
    "\t\tself.model = model.cuda() if self.use_cuda else model\n",
    "\n",
    "\t\t# freeze all layer weights\n",
    "\t\tfor param in self.model.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\n",
    "\t\t# modifying the classifier layer\n",
    "\t\tnum_features = self.model.fc.in_features\n",
    "\t\tself.model.fc = nn.Linear(num_features, num_class)\n",
    "\n",
    "\t\t# we choose to unfreeze the weights of parameters following(but not including) the Conv2d_4a_3x3 layer\n",
    "\t\t# act as a testing mechanism\n",
    "\t\tct = []\n",
    "\t\t# loop through through the layers\n",
    "\t\tfor name, child in self.model.named_children():\n",
    "\t\t\tif \"Conv2d_4a_3x3\" in ct:\n",
    "\t\t\t\tfor params in child.parameters():\n",
    "\t\t\t\t\tparams.requires_grad = True\n",
    "\t\t\t\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\treturn(self.model(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 train_alt.py\n",
    "function for training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "import math\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import time\n",
    "from pretrained_inceptionv3_alt import pretrained_inception_v3\n",
    "from custom_dset_new_alt import custom_dset, train_val_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "def train(data_dir, save_dir, num_class, num_epoch = 20,\\\n",
    "\tbs = 4, lr = 1e-3, use_cuda = False, num_workers = 1,\\\n",
    "\tname = 'model', train_prop = 0.7, val_prop = 0.2,\n",
    "\tstep_size = 4, gamma = 0.1):\n",
    "\t\"\"\"\n",
    "\tparams:\n",
    "\tdata_dir: where the image folder is stored\n",
    "\tsave_dir: where the model should be saved after/during training\n",
    "\tnumber_class: the number of classes to predict\n",
    "\tnum_epoch (20 by default): the number of epochs to train\n",
    "\tbs (4 by default): the batch size\n",
    "\tlr (0.001 by default): the starting learning rate\n",
    "\tuse_cude(false by default): boolean, wether to use the GPU\n",
    "\tnum_workers (1 by dfault): the number of workers to use for the computation,\n",
    "\t\t\t\tnote that if use_cude = True, it should be set to equal 1\n",
    "\tname ('model' by default): name of the model when it is saved\n",
    "\ttrain_prop (0.7 by default): the propotion of the data used for trainning\n",
    "\tval_prop (0.2 by default): the propotion of the data used for validation\n",
    "\tstep_size (4 by default): the frequency for learning rate decay\n",
    "\tgamma (0.1 by default): the factor by which the learning rate decays\n",
    "\t\"\"\"\n",
    "\n",
    "\t# checkpoint beginning time\n",
    "\tbegin = time.time()\n",
    "\n",
    "\t# instantiate the vgg model\n",
    "\tmodel = pretrained_inception_v3(num_class, use_cuda)\n",
    "\n",
    "\t# we check if the save_dir exists, if not, we create it\n",
    "\tif not os.path.isdir(save_dir):\n",
    "\t\tos.mkdir(save_dir)\n",
    "\n",
    "\t# define the model path\n",
    "\tmodelpath = os.path.join(save_dir, '{}.pt'.format(name))\n",
    "\n",
    "\t# in the case of paused training, do we wish to continue training?\n",
    "\tif use_cuda:\n",
    "\t\tmodel = model.cuda()\n",
    "\n",
    "\t# setting up the loss and accuracy variables\n",
    "\tloss_record = {'train': np.zeros(num_epoch), 'val': np.zeros(num_epoch)}\n",
    "\tacc_record = {'train': np.zeros(num_epoch), 'val': np.zeros(num_epoch)}\n",
    "\n",
    "\t# setting up the loss function and optimisation method\n",
    "\t# we use the cross entropy for the multiclass classification task\n",
    "\tloss_fun = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
    "\t# we use stochastic gradient descend for optimisation\n",
    "\toptim = SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "\t# we apply learning rate decay\n",
    "\texp_lr_scheduler = lr_scheduler.StepLR(optim, step_size = step_size, gamma = gamma)\n",
    "\n",
    "\t# split the dataset into train, val and test\n",
    "\ttest_prop = 1 - train_prop - val_prop\n",
    "\ttrain_data, val_data, test_data = train_val_test_split(data_dir, train_prop, val_prop, test_prop)\n",
    "\n",
    "\t# saving the test data. We save the test data first in case we want to terminate the training early\n",
    "\twith open('{}.pickle'.format(os.path.join(save_dir, 'test_data')), 'wb') as handle:\n",
    "\t\tpickle.dump(test_data, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\t# set up the datasets\n",
    "\tdset = {'train': custom_dset(data_files = train_data, transform = 'train'),\n",
    "\t\t\t'val': custom_dset(data_files = val_data, transform = 'val')}\n",
    "\n",
    "\t# setting up a random weighted sampler to ensure the classes are balanced in the training phase\n",
    "\t# calculating weights for the dset indices according to their respective class\n",
    "\ttrain_class_sample_count = Counter(dset['train'].labels)\n",
    "\tsorted_train_class_sample_count = [train_class_sample_count[key] for key in sorted(train_class_sample_count.keys())]\n",
    "\tweights = 100000./torch.tensor(sorted_train_class_sample_count, dtype = torch.float)\n",
    "\tsamples_weights = [weights[label] for label in dset['train'].labels]\n",
    "\t# the sampler\n",
    "\tsampler = WeightedRandomSampler(weights=samples_weights,\n",
    "\t\t\t\t\t\t\t\t\tnum_samples=len(samples_weights),\n",
    "\t\t\t\t\t\t\t\t\treplacement=True)\n",
    "\t# the dataloaders, we create 2 dataloaders for the train and val phase seperately\n",
    "\tdataloaders = {'train': DataLoader(dset['train'], batch_size = bs, sampler = sampler, num_workers = num_workers, pin_memory = False),\n",
    "\t\t\t\t\t'val': DataLoader(dset['val'], batch_size = bs, shuffle = True, num_workers = num_workers, pin_memory = False)}\n",
    "\n",
    "\t# create variables for storing best performing weights during training\n",
    "\tbest_model_wts = model.state_dict()\n",
    "\tbest_acc = 0.0\n",
    "\n",
    "\t# iterate through the training epochs\n",
    "\tfor epoch in range(num_epoch):\n",
    "\t\tprint('Epoch {}/{}'.format(epoch, num_epoch - 1))\n",
    "\t\tprint('-' * 10)\n",
    "\n",
    "\t\t# recording the running performance each epoch\n",
    "\n",
    "\t\t# each epoch will have a training and validation phase\n",
    "\t\tfor phase in ['train', 'val']:\n",
    "\n",
    "\t\t\t# recording the dataset size\n",
    "\t\t\trunning_loss = 0.0\n",
    "\t\t\trunning_corrects = 0\n",
    "\t\t\tsize = 0\n",
    "\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\texp_lr_scheduler.step()\n",
    "\t\t\t\t# setting model to trainning mode\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\telse:\n",
    "\t\t\t\t# setting model to validation mode\n",
    "\t\t\t\tmodel.eval()\n",
    "\n",
    "\t\t\t# now we iterate over the data\n",
    "\t\t\tfor inputs, labels in tqdm(dataloaders[phase]):\n",
    "\t\t\t\t# counting how many images is contained in this batch\n",
    "\t\t\t\tbatch_count = labels.size(0)\n",
    "\t\t\t\t# if GPU is used, cudafy inputs\n",
    "\t\t\t\tif use_cuda:\n",
    "\t\t\t\t\tinputs = inputs.cuda()\n",
    "\t\t\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t\t\t# zero the parameter gradients\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\t\t\t# feed inputs into the model\n",
    "\t\t\t\toutput = model(inputs)\n",
    "\n",
    "\n",
    "\t\t\t\t# calculate the loss and prediction performance statistics\n",
    "\t\t\t\tif type(output) == tuple:\n",
    "\t\t\t\t\toutput, _ = output\n",
    "\t\t\t\t_, preds = torch.max(output.data, 1)\n",
    "\t\t\t\tloss = loss_fun(output, labels)\n",
    "\t\t\t\trunning_loss += loss\n",
    "\t\t\t\trunning_corrects += preds.eq(labels.view_as(preds)).sum()\n",
    "\n",
    "\t\t\t\t# backprop and optimise if in training stage\n",
    "\t\t\t\tif phase == 'train':\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptim.step()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# update dataset size\n",
    "\t\t\t\tsize += batch_count\n",
    "\n",
    "\t\t\tepoch_loss = running_loss / size\n",
    "\t\t\tepoch_acc = running_corrects.item() / size\n",
    "\n",
    "\t\t\t# recording the historical performance\n",
    "\t\t\tloss_record[phase][epoch] = epoch_loss\n",
    "\t\t\tacc_record[phase][epoch] = epoch_acc\n",
    "\n",
    "\t\t\tprint('{} loss: {:.4F} Acc: {:.4F}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "\t\t\t# deep copy and save the model if best performance\n",
    "\t\t\tif phase == 'val' and epoch_acc > best_acc:\n",
    "\t\t\t\tbest_acc = epoch_acc\n",
    "\t\t\t\tbest_model_wts = model.state_dict()\n",
    "\t\t\t\ttorch.save(model.state_dict(), os.path.join(save_dir, '{}.pt'.format(name)))\n",
    "\n",
    "\t# saving the record performances\n",
    "\twith open('{}.pickle'.format(os.path.join(save_dir, name + '_loss_performances')), 'wb') as handle:\n",
    "\t\tpickle.dump(loss_record, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\twith open('{}.pickle'.format(os.path.join(save_dir, name + '_acc_performances')), 'wb') as handle:\n",
    "\t\tpickle.dump(acc_record, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\t# calculating time elapsed\n",
    "\ttime_elapsed = time.time() - begin\n",
    "\tprint('Training complete in {:.0F}m {:0F}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\tprint('Best validation acc: {:4F}'.format(best_acc))\n",
    "\n",
    "\t# load the best model weights\n",
    "\tmodel.load_state_dict(best_model_wts)\n",
    "\treturn(loss_record, acc_record, model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 test_alt.py\n",
    "function for testing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "import math\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import time\n",
    "from pretrained_inceptionv3_alt import pretrained_inception_v3\n",
    "from custom_dset_new_alt import custom_dset, train_val_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "def test(model, test_files, bs):\n",
    "\t\"\"\"\n",
    "\tmodel : the model to be tested\n",
    "\ttest_files : the directories to the test images, output of the train_val_test_split function\n",
    "\tbs : batch size\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\t# set model to eval mode\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# cudafy model if specified\n",
    "\tif use_cuda:\n",
    "\t\tmodel = model.cuda()\n",
    "\n",
    "\t# recording the running performance and the dataset size\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\tsize = 0\n",
    "\n",
    "\t# set up the loss function\n",
    "\tloss_fun = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
    "\n",
    "\t# set up the datasets\n",
    "\tdset = custom_dset(data_files = test_files, transform = 'val')\n",
    "\n",
    "\t# set up the dataloader\n",
    "\tdataloader = DataLoader(dset, batch_size = bs, shuffle = True, num_workers = 1, pin_memory = False)\n",
    "\n",
    "\t# now iterate over the images to make predictions\n",
    "\tfor inputs, labels in tqdm(dataloader):\n",
    "\t\t# cudafy inputs and labels if specified\n",
    "\t\tif use_cuda:\n",
    "\t\t\tinputs = inputs.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\t# counting how many images is contained in this batch\n",
    "\t\tbatch_count = labels.size(0)\n",
    "\t\t# Forward pass\n",
    "\t\toutput = model(inputs)\n",
    "\n",
    "\t\t# calculate the loss and prediction performance statistics\n",
    "\t\tif type(output) == tuple:\n",
    "\t\t\toutput, _ = output\n",
    "\t\t\t_, preds = torch.max(output.data, 1)\n",
    "\t\t\tloss = loss_fun(output, labels)\n",
    "\t\t\trunning_loss += loss\n",
    "\t\t\trunning_corrects += preds.eq(labels.view_as(preds)).sum()\n",
    "\n",
    "\t\n",
    "\t\t# update dataset size\n",
    "\t\tsize += batch_count\n",
    "\n",
    "\t# compute the model's performance\n",
    "\tmodel_loss = running_loss / size\n",
    "\tmodel_acc = running_corrects / size\n",
    "\n",
    "\treturn(model_loss, model_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 execute_train_alt.py\n",
    "python script used for training the CNN in linux terminal with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_alt import train\n",
    "from custom_dset_new_alt import train_val_test_split, custom_dset\n",
    "from pretrained_inceptionv3_alt import pretrained_inception_v3\n",
    "\n",
    "\n",
    "# setting up variables\n",
    "data_dir = '../../data/images/'\n",
    "save_dir = '../../data/CNN_model_landtype_alt/'\n",
    "num_class = 6\n",
    "bs = 32\n",
    "name = 'model_alt'\n",
    "num_epoch = 8\n",
    "lr = 1e-5\n",
    "step_size = 4\n",
    "gamma = 0.2\n",
    "\n",
    "\n",
    "# run\n",
    "loss_record, acc_record, model, test_data = train(data_dir = data_dir, save_dir = save_dir, num_class = 6, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tnum_epoch = num_epoch, bs = bs, lr = lr, step_size = step_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tgamma = gamma, use_cuda = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
